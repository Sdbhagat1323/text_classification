{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### @ morph, for the YSDA ML Trainings 18 October, 2014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data_dir = 'tradeshift/'\n",
    "!mkdir {data_dir}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!wget 'https://kaggle2.blob.core.windows.net/competitions-data/kaggle/3984/train.csv.gz?sv=2012-02-12&se=2014-10-21T00%3A06%3A50Z&sr=b&sp=r&sig=cupgPW%2BU6BpdsnrykcEBBRqLEW565pXYQ6k%2FSc0Me1M%3D' -O {data_dir + 'train.csv.gz'}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!wget 'https://kaggle2.blob.core.windows.net/competitions-data/kaggle/3984/test.csv.gz?sv=2012-02-12&se=2014-10-21T00%3A09%3A52Z&sr=b&sp=r&sig=YLQCFyAdhIRnz2o4p24zRssUjHYjQ1xOHuTKFsdLxu8%3D' -O {data_dir + 'test.csv.gz'}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!wget 'https://kaggle2.blob.core.windows.net/competitions-data/kaggle/3984/trainLabels.csv.gz?sv=2012-02-12&se=2014-10-21T00%3A11%3A04Z&sr=b&sp=r&sig=%2Bm9sbZYXOY8L80d1PJEdumGPXvkQby2rpkVOf1fvjUM%3D' -O {data_dir + 'trainLabels.csv.gz'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unpack"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "!gunzip {data_dir + '*.gz'}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!ls -l -h {data_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big Data -- Sample Data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(data_dir + 'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1700000, 146)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(170000, 146)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_size = 170000\n",
    "ratio = train.shape[0] / sample_size\n",
    "\n",
    "train_sample = train[\n",
    "    [hash(id) % ratio == 0 for id in train['id']]\n",
    "]\n",
    "\n",
    "train_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample.to_csv(data_dir + 'train_sample.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free memory\n",
    "\n",
    "del train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try to make something useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_dir = 'tradeshift/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample = pd.read_csv(data_dir + 'train_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv(data_dir + 'trainLabels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'id', u'y1', u'y2', u'y3', u'y4', u'y5', u'y6', u'y7', u'y8', u'y9', u'y10', u'y11', u'y12', u'y13', u'y14', u'y15', u'y16', u'y17', u'y18', u'y19', u'y20', u'y21', u'y22', u'y23', u'y24', u'y25', u'y26', u'y27', u'y28', u'y29', u'y30', u'y31', u'y32', u'y33'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_labels = pd.merge(train_sample, labels, on = 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(170000, 179)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_with_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'x': 145, 'y': 33, 'i': 1})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter([name[0] for name in train_with_labels.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del labels\n",
    "del train_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(data_dir + 'test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical values encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x3 40882\n",
      "x4 5019\n",
      "x34 48090\n",
      "x35 6797\n",
      "x61 78363\n",
      "x64 49408\n",
      "x65 7035\n",
      "x91 27960\n",
      "x94 37786\n",
      "x95 5086\n",
      "(170000, 135) (170000, 306426) (545082, 135) (545082, 306426)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "X_numerical = []\n",
    "X_test_numerical = []\n",
    "\n",
    "vec = DictVectorizer()\n",
    "\n",
    "names_categorical = []\n",
    "\n",
    "train_with_labels.replace('YES', 1, inplace = True)\n",
    "train_with_labels.replace('NO', 0, inplace = True)\n",
    "train_with_labels.replace('nan', np.NaN, inplace = True)\n",
    "\n",
    "test.replace('YES', 1, inplace = True)\n",
    "test.replace('NO', 0, inplace = True)\n",
    "test.replace('nan', np.NaN, inplace = True)\n",
    "\n",
    "\n",
    "for name in train_with_labels.columns :    \n",
    "    if name.startswith('x') :\n",
    "        column_type, _ = max(Counter(map(lambda x: str(type(x)), train_with_labels[name])).items(), key = lambda x: x[1])\n",
    "        \n",
    "        # LOL expression\n",
    "        if column_type == str(str) :\n",
    "            train_with_labels[name] = map(str, train_with_labels[name])\n",
    "            test[name] = map(str, test[name])\n",
    "\n",
    "            names_categorical.append(name)\n",
    "            print name, len(np.unique(train_with_labels[name]))\n",
    "        else :\n",
    "            X_numerical.append(train_with_labels[name].fillna(-999))\n",
    "            X_test_numerical.append(test[name].fillna(-999))\n",
    "        \n",
    "X_numerical = np.column_stack(X_numerical)\n",
    "X_test_numerical = np.column_stack(X_test_numerical)\n",
    "\n",
    "X_sparse = vec.fit_transform(train_with_labels[names_categorical].T.to_dict().values())\n",
    "X_test_sparse = vec.transform(test[names_categorical].T.to_dict().values())\n",
    "\n",
    "print X_numerical.shape, X_sparse.shape, X_test_numerical.shape, X_test_sparse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_numerical = np.nan_to_num(X_numerical)\n",
    "X_test_numerical = np.nan_to_num(X_test_numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "joblib.dump(\n",
    "    (X_numerical, X_sparse, X_test_numerical, X_test_sparse),\n",
    "    data_dir + 'X.dump',\n",
    "    compress = 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying to predict something"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build two level classifier, first train base level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print'. Did you mean print(\"Build meta\")? (<ipython-input-7-a1a3af83c28b>, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-a1a3af83c28b>\"\u001b[0;36m, line \u001b[0;32m20\u001b[0m\n\u001b[0;31m    print \"Build meta\"\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print(\"Build meta\")?\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, f1_score, log_loss, make_scorer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.cross_validation import cross_val_score, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "log_loss_scorer = make_scorer(log_loss, needs_proba = True)\n",
    "\n",
    "y_columns = [name for name in train_with_labels.columns if name.startswith('y')]\n",
    "\n",
    "X_numerical_base, X_numerical_meta, X_sparse_base, X_sparse_meta, y_base, y_meta = train_test_split(\n",
    "    X_numerical, \n",
    "    X_sparse, \n",
    "    train_with_labels[y_columns].values,\n",
    "    test_size = 0.5\n",
    ")\n",
    "\n",
    "X_meta = [] \n",
    "X_test_meta = []\n",
    "\n",
    "print \"Build meta\"\n",
    "\n",
    "for i in range(y_base.shape[1]) :\n",
    "    print i\n",
    "    \n",
    "    y = y_base[:, i]\n",
    "    if len(np.unique(y)) == 2 : \n",
    "        rf = RandomForestClassifier(n_estimators = 10, n_jobs = 1)\n",
    "        rf.fit(X_numerical_base, y)\n",
    "        X_meta.append(rf.predict_proba(X_numerical_meta))\n",
    "        X_test_meta.append(rf.predict_proba(X_test_numerical))\n",
    "\n",
    "        svm = LinearSVC()\n",
    "        svm.fit(X_sparse_base, y)\n",
    "        X_meta.append(svm.decision_function(X_sparse_meta))\n",
    "        X_test_meta.append(svm.decision_function(X_test_sparse))\n",
    "        \n",
    "X_meta = np.column_stack(X_meta)\n",
    "X_test_meta = np.column_stack(X_test_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print X_meta.shape, X_test_meta.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here train meta level and get predictions for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-94710b8babb3>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-94710b8babb3>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    print \"%d is constant like: %f\" % (i, constant_pred)\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "p_test = []\n",
    "\n",
    "for i in range(y_base.shape[1]) :\n",
    "    y = y_meta[:, i]\n",
    "\n",
    "    constant = Counter(y)\n",
    "    constant = constant[0] < 4 or constant[1] < 4\n",
    "    \n",
    "    predicted = None\n",
    "    \n",
    "    if constant :\n",
    "        # Best constant\n",
    "        constant_pred = np.mean(list(y_base[:, i]) + list(y_meta[:, i]))\n",
    "        \n",
    "        predicted = np.ones(X_test_meta.shape[0]) * constant_pred\n",
    "        print \"%d is constant like: %f\" % (i, constant_pred)\n",
    "    else :\n",
    "        rf = RandomForestClassifier(n_estimators=30, n_jobs = 1)\n",
    "        rf.fit(np.hstack([X_meta, X_numerical_meta]), y)\n",
    "\n",
    "        predicted = rf.predict_proba(np.hstack([X_test_meta, X_test_numerical]))\n",
    "\n",
    "        predicted = predicted[:, 1]\n",
    "        \n",
    "        rf = RandomForestClassifier(n_estimators=30, n_jobs = 1)\n",
    "        scores = cross_val_score(rf, np.hstack([X_meta, X_numerical_meta]), y, cv = 4, n_jobs = 1, scoring = log_loss_scorer)\n",
    "\n",
    "        print i, 'RF log-loss: %.4f ± %.4f, mean = %.6f' %(np.mean(scores), np.std(scores), np.mean(predicted))\n",
    "\n",
    "    \n",
    "    p_test.append(\n",
    "        predicted\n",
    "    )\n",
    "    \n",
    "p_test = np.column_stack(p_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "def save_predictions(name, ids, predictions) :\n",
    "    out = gzip.open(name, 'w')\n",
    "    print >>out, 'id_label,pred'\n",
    "    for id, id_predictions in zip(test['id'], p_test) :\n",
    "        for y_id, pred in enumerate(id_predictions) :\n",
    "            if pred == 0 or pred == 1 :\n",
    "                pred = str(int(pred))\n",
    "            else :\n",
    "                pred = '%.6f' % pred\n",
    "            print >>out, '%d_y%d,%s' % (id, y_id + 1, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_predictions('quick_start.csv.gz', test['id'].values, p_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l -h  quick_start*.csv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Public result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick start on 10% of train - 0.0212323"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
